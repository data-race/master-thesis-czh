In the field of deep learning, with the improvement of the performance of deep neural network models, the complexity and size of the models are also increasing. A single computing device is no longer sufficient to meet the training requirements of large neural network models. Therefore, algorithm researchers usually use model parallelism to partition large models to multiple computing devices for distributed training.

In mainstream deep learning frameworks, the partitioning of models still relies on manual processing by users. Due to the complexity of model structure, coupled with the heterogeneity of underlying devices, even experienced researchers find it challenging to partition models manually.

Existing methods for partitioning models have used reinforcement learning, heuristic algorithms, and constrained optimization problems, among others. However, these methods still have some limitations, such as the lack of consideration for the underlying hardware environment and insufficient accuracy in modeling the model training process.

This article proposes a training framework \sys{} for large deep neural network models, which includes the following main work:

\begin{itemize}
    \item  Proposing an automated PyTorch model analysis method that can perform model structural analysis on generic PyTorch models, extract the intermediate representation of the model calculation graph, and analyze metadata such as calculation time and memory usage for each node in the calculation graph.
    \item  Proposing a modeling method for inter-device communication costs at the underlying computing devices for heterogeneous communication linkages, to facilitate automatic point-to-point communication testing between devices and modeling the communication costs between devices.
    \item Proposing a model partitioning method based on constrained optimization solutions for partitioning models based on the calculation graph metadata and inter-device communication costs. The method constructs an optimization problem and solves it for model partitioning.
    \item Conducting experimental evaluations of \sys{} in real-world scenarios which showed that \sys{} can effectively improve the training efficiency of large models and reduce training time compared to existing methods.
\end{itemize}