在深度学习领域，随着深度神经网络模型性能的提升，模型的复杂度和大小也在不断增加。单个计算设备已经无法满足大型神经网络模型的训练需求。因此，算法研究者通常使用模型并行化技术，将大型模型划分到多个计算设备上，进行分布式训练。
主流的深度学习框架中，对于模型的划分，仍然依赖使用者手动进行。由于模型结构复杂，加上底层设备的异构性，即使是对于有丰富经验的研究者，手动划分模型也是非常困难的任务。

现有的工作通过强化学习、启发式算法、构建约束优化问题等方法划分模型，但是目前的方法仍然存在一些不足。例如缺少对于底层硬件环境的考虑，对模型训练过程的建模不够精确等。
本文提出了一种针对大型深度神经网络模型的训练框架：\sys{}。具体而言，本文的主要工作包括：
\begin{itemize}
	\item 提出了一种自动化的PyTorch模型分析方法，可以从通用的PyTorch模型中提取出模型计算图的中间表示，并使用静态分析和动态分析的方法分析计算图中每个节点的计算时间和内存占用等元信息。
	\item 提出了一种对底层计算设备之间通信代价的建模方法：针对设备之间可能存在的异构通信链路，自动对设备之间进行点对点通信测试，并建模设备之间的通信代价。
	\item 提出了基于约束优化求解的模型划分方法：基于计算图元信息和设备之间的通信代价模型，构建约束优化问题并进行求解，对模型进行划分。
	\item 在真实场景下对\sys{}进行了实验评估，结果表明，和现有方法对比，\sys{}可以有效提升大型模型的训练效率，缩短训练时间。
\end{itemize}
